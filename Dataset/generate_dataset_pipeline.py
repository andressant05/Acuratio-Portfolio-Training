import os
import json
import time
import difflib
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from fuzzywuzzy import fuzz

# üí¨ Ejemplos que sirven de gu√≠a (few-shots) para el modelo LLM
EJEMPLOS_TIPICOS = {
    "definition": [
        {
            "prompt": "¬øQu√© es un equipo de tratamiento t√©rmico seg√∫n la norma?",
            "response": "Un equipo de tratamiento t√©rmico es un sistema utilizado para calentar materiales mediante combustibles o electricidad, tal como se especifica en el cap√≠tulo 3 de la norma."
        },
        {
            "prompt": "¬øQu√© se entiende por peligro mec√°nico en el contexto de esta norma?",
            "response": "Un peligro mec√°nico incluye el movimiento de maquinaria, la expulsi√≥n de materiales, o el fallo estructural que puede causar da√±o f√≠sico."
        },
    ],
    "justification": [
        {
            "prompt": "¬øPor qu√© es importante evitar el retroceso de llamaradas en los equipos?",
            "response": "Porque el retroceso puede causar explosiones o da√±os al sistema, lo cual pone en riesgo a los operadores y al entorno."
        },
        {
            "prompt": "¬øPor qu√© deben marcarse adecuadamente las tuber√≠as de conducci√≥n?",
            "response": "Porque un marcaje adecuado permite identificar peligros como temperaturas extremas o presi√≥n alta, ayudando a prevenir accidentes."
        },
    ],
    "hypothetical": [
        {
            "prompt": "Si los equipos carecen de accesos seguros para mantenimiento, ¬øqu√© podr√≠a ocurrir?",
            "response": "Los operarios podr√≠an exponerse a riesgos f√≠sicos al intentar operar o reparar el equipo sin medios de acceso adecuados."
        },
        {
            "prompt": "Si se omiten medidas contra la acumulaci√≥n de gases t√≥xicos en el interior del equipo, ¬øcu√°l podr√≠a ser el resultado?",
            "response": "Podr√≠a producirse una intoxicaci√≥n o incluso una explosi√≥n, especialmente si no hay salidas de emergencia o ventilaci√≥n adecuada."
        },
    ],
    "comparison": [
        {
            "prompt": "¬øCu√°l es la diferencia entre los techos accesibles y no accesibles seg√∫n la norma?",
            "response": "Los techos accesibles deben contar con rampas seguras y barandillas, mientras que los no accesibles deben estar marcados o protegidos para evitar el acceso."
        },
    ],
    "scenario": [
        {
            "prompt": "Un operador necesita acceder a una zona elevada del equipo durante una inspecci√≥n. ¬øQu√© requisitos debe cumplir esa zona?",
            "response": "Debe ser accesible mediante rampas seguras, estar equipada con barandillas, y cumplir los requisitos de seguridad indicados en el apartado 5.2.11."
        },
    ],
    "role": [
        {
            "prompt": "¬øCu√°l es la responsabilidad del usuario en cuanto a la instalaci√≥n del equipo seg√∫n la norma?",
            "response": "El usuario debe asegurarse de que el lugar de instalaci√≥n cumple con los requisitos de seguridad y que se dispone de ventilaci√≥n adecuada."
        },
        {
            "prompt": "¬øQu√© debe hacer el fabricante respecto al manual de instrucciones?",
            "response": "Debe proporcionar un manual t√©cnico que incluya los m√©todos de operaci√≥n, mantenimiento, protecci√≥n personal requerida y riesgos previstos."
        },
    ]
}

# üîß Cliente para comunicarse con el modelo LLM (en este caso, llama3 local)
cliente_llm = OpenAI(api_key="EMPTY", base_url="http://localhost:9000/v1")

# üéØ Instrucci√≥n general que gu√≠a al modelo
INSTRUCCION_SISTEMA = (
    "You are a professional dataset generator for machine learning. Your goal is to produce instruction-response pairs "
    "that are strictly based on the input document chunk. Never fabricate information or introduce concepts not explicitly supported by the text. "
    "Responses must be formal, technical, and reference the language or logic of the document exactly. Avoid summaries, advice, or motivation."
)

# üî™ Divide un texto largo en fragmentos manejables de cierto tama√±o
def dividir_texto(texto_completo, max_caracteres=2000):
    oraciones = texto_completo.split(". ")
    fragmentos, actual = [], ""
    for oracion in oraciones:
        if len(actual) + len(oracion) < max_caracteres:
            actual += (oracion + ". ")
        else:
            fragmentos.append(actual.strip())
            actual = oracion + ". "
    if actual:
        fragmentos.append(actual.strip())
    return fragmentos

# üß± Construye el prompt con los ejemplos + texto a analizar
def construir_prompt(ejemplos, fragmento, tipo, n=2):
    ejemplos_str = "\n\n".join(json.dumps(x, ensure_ascii=False) for x in ejemplos)
    return f"""{INSTRUCCION_SISTEMA}
--- FEW-SHOT {tipo.upper()} ---
{ejemplos_str}

--- DOCUMENTO ---
{fragmento}

Now generate {n} instruction-response pairs in JSONL format. Base every detail strictly on the document content. Do not invent facts.
"""

# üîÆ Env√≠a el prompt al modelo y obtiene la respuesta
def llamar_llm(prompt):
    respuesta = cliente_llm.chat.completions.create(
        model="meta-llama/Llama-3.3-70B-Instruct",
        messages=[
            {"role": "system", "content": INSTRUCCION_SISTEMA},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2,
        max_tokens=1024
    )
    return respuesta.choices[0].message.content

# üì§ Interpreta la respuesta del modelo y extrae los pares v√°lidos
def interpretar_respuesta(texto_respuesta, tipo):
    resultado = []
    for linea in texto_respuesta.strip().splitlines():
        try:
            item = json.loads(linea)
            if "prompt" in item and "response" in item:
                item["type"] = tipo
                resultado.append(item)
        except json.JSONDecodeError:
            continue
    return resultado

# ü§ñ Genera pares QA para cada fragmento de texto
def generar_qa_para_fragmento(texto, preguntas_por_tipo=1, tipos_requeridos=None):
    if tipos_requeridos is None:
        tipos_requeridos = ["definition", "justification", "scenario", "role"]
    resultado = []
    for tipo in tipos_requeridos:
        prompt = construir_prompt(EJEMPLOS_TIPICOS[tipo], texto, tipo, n=preguntas_por_tipo)
        respuesta = llamar_llm(prompt)
        resultado.extend(interpretar_respuesta(respuesta, tipo))
    return resultado

# üßπ Elimina duplicados similares entre prompts
def eliminar_duplicados(lista, umbral=90):
    unicos = []
    for item in lista:
        if any(fuzz.ratio(item["prompt"], otro["prompt"]) > umbral for otro in unicos):
            continue
        unicos.append(item)
    return unicos

# üîÅ Compara prompts para evitar repetir
def es_duplicado(prompt_nuevo, prompts_existentes, umbral=0.9):
    for existente in prompts_existentes:
        if difflib.SequenceMatcher(None, prompt_nuevo.lower(), existente.lower()).ratio() > umbral:
            return True
    return False

# üîç Procesa un archivo JSON, genera pares y adjunta metadatos
def procesar_archivo(ruta_json):
    datos = json.loads(ruta_json.read_text(encoding="utf-8"))
    texto = "\n\n".join(datos["chunks"])
    fragmentos = dividir_texto(texto, max_caracteres=3000)
    fragmentos_filtrados = [f for f in fragmentos if len(f.strip()) > 300]
    resultado = []
    prompts_vistos = set()

    for idx, frag in enumerate(fragmentos_filtrados):
        pares = generar_qa_para_fragmento(frag, preguntas_por_tipo=1)
        for qa in pares:
            prompt = qa["prompt"]
            if prompt in prompts_vistos or es_duplicado(prompt, prompts_vistos):
                continue
            prompts_vistos.add(prompt)
            resultado.append({
                "context": frag.strip(),
                "prompt": prompt,
                "response": qa["response"],
                "metadata": {
                    "source_file": ruta_json.name,
                    "chunk_index": idx
                }
            })

    return eliminar_duplicados(resultado)

# üß† Ejecuci√≥n principal del pipeline: detecta archivos nuevos, procesa y graba
def main():
    inicio = time.time()
    carpeta = Path("processed_chunks")
    archivos = sorted(carpeta.glob("*.json"))
    carpeta_salida = Path("temp_outputs")
    carpeta_salida.mkdir(exist_ok=True)

    ruta_procesados = Path(".processed_files.json")
    archivos_procesados = set()
    if ruta_procesados.exists():
        with open(ruta_procesados, "r", encoding="utf-8") as f:
            archivos_procesados = set(json.load(f))

    nuevos_archivos = [f for f in archivos if f.name not in archivos_procesados]
    print(f"üìÅ Archivos nuevos a procesar: {len(nuevos_archivos)}")

    resultados = []
    with ThreadPoolExecutor(max_workers=8) as pool:
        tareas = {pool.submit(procesar_archivo, a): a for a in nuevos_archivos}
        for t in as_completed(tareas):
            archivo = tareas[t]
            try:
                pares = t.result()
                print(f"‚úÖ {archivo.name}: {len(pares)} QAs generadas")
                resultados.extend(pares)

                salida = carpeta_salida / f"{archivo.stem}_qas.jsonl"
                with open(salida, "w", encoding="utf-8") as f:
                    for qa in pares:
                        f.write(json.dumps(qa, ensure_ascii=False) + "\n")

                archivos_procesados.add(archivo.name)
                with open(ruta_procesados, "w", encoding="utf-8") as f:
                    json.dump(sorted(archivos_procesados), f, ensure_ascii=False, indent=2)

            except Exception as e:
                print(f"‚ùå Error al procesar {archivo.name}: {e}")

    resultado_final = eliminar_duplicados(resultados)
    with open("context_full_dataset.jsonl", "w", encoding="utf-8") as f:
        for qa in resultado_final:
            f.write(json.dumps(qa, ensure_ascii=False) + "\n")

    duracion = time.time() - inicio
    print(f"‚úÖ ¬°Listo! Dataset generado en context_full_dataset.jsonl")
    print(f"üïê Tiempo total: {duracion/60:.2f} minutos")

if __name__ == "__main__":
    main()

    
    
"""
# ‚úÖ Acuratio Model Training ‚Äî Handoff Notes

Hola equipo üëã  
Aqu√≠ les dejo una gu√≠a clara para continuar el trabajo f√°cilmente.

---

## üîÅ Generaci√≥n de Dataset (`generate_dataset_pipeline.py`)

### C√≥mo usarlo:
1. Coloca nuevos archivos `.json` dentro de la carpeta `processed_chunks/`.
2. Ejecuta:
   python generate_dataset_pipeline.py
3. El sistema procesar√° **s√≥lo los archivos nuevos**.
4. El dataset final estar√° en: context_full_dataset.jsonl

> üß† Se guarda un archivo `.processed_files.json` para llevar control de qu√© archivos ya fueron procesados.

"""
